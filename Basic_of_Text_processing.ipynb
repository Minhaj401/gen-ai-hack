{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minhaj401/gen-ai-hack/blob/main/Basic_of_Text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeyleFAkZv6v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Text from URL"
      ],
      "metadata": {
        "id": "H0e80caWbGH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.geeksforgeeks.org/nlp/semantic-roles-in-nlp/\"\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Remove script and style elements\n",
        "for tag in soup([\"script\", \"style\"]):\n",
        "    tag.decompose()\n",
        "\n",
        "text = soup.get_text(separator=\" \")\n",
        "print(\"Raw text sample:\\n\", text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWNyJbU2bKrr",
        "outputId": "d3d60f0b-8873-4095-c292-ca71787a9a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw text sample:\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " Semantic Roles in NLP - GeeksforGeeks \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " Skip to content \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " Courses DSA / Placements ML & Data Science Development Cloud / DevOps Programming Languages All Courses Tutorials Python Java ML & Data Science Programming Languages Web Development CS Subjects DevOps Software and Tool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning: Remove unwanted space"
      ],
      "metadata": {
        "id": "Z9FRw9Itb5lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# text obtained from BeautifulSoup\n",
        "clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(\"Cleaned text sample:\\n\", clean_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifa7ENfHb39j",
        "outputId": "d6738ce0-80df-48bc-cca6-4be771162fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text sample:\n",
            " On 12 March 2024, Dr. Anil Kumar from IIIT Kottayam emailed Amazon India about a delayed delivery of ₹4,999. The order was placed in Kochi using the Prime app at 10:30 am. Later, he mentioned that Apple Watch Series 8 was promised within 2 days but arrived after 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"On 12 March 2024, Dr. Anil Kumar from IIIT Kottayam emailed Amazon India about a delayed delivery of ₹4,999. The order was placed in Kochi using the Prime app at 10:30 am. Later, he mentioned that Apple Watch Series 8 was promised within 2 days but arrived after 5.\""
      ],
      "metadata": {
        "id": "JExoE3IzeszR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "mp5Og747cKri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb8NpQsscZJ8",
        "outputId": "9105dac3-e3c8-4dab-e326-09b1f76f6139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\nTokens sample:\\n\", tokens[:20])\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmpGNZ3fcJkb",
        "outputId": "c717a82e-b6ae-4d17-e15c-2fb08de85d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens sample:\n",
            " ['On', '12', 'March', '2024', ',', 'Dr.', 'Anil', 'Kumar', 'from', 'IIIT', 'Kottayam', 'emailed', 'Amazon', 'India', 'about', 'a', 'delayed', 'delivery', 'of', '₹4,999']\n",
            "54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E0LiV7ckczrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging"
      ],
      "metadata": {
        "id": "WWXByncYcqZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_teRrkdvc1vm",
        "outputId": "5107e758-960a-49ec-e7c5-0f2416b38d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(\"\\nPOS tags sample:\\n\", pos_tags[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMai7mcqcjQb",
        "outputId": "98feda20-94db-49eb-9c2f-83bfa6a749e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS tags sample:\n",
            " [('On', 'IN'), ('12', 'CD'), ('March', 'NNP'), ('2024', 'CD'), (',', ','), ('Dr.', 'NNP'), ('Anil', 'NNP'), ('Kumar', 'NNP'), ('from', 'IN'), ('IIIT', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "# POS tags already computed earlier\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "print(\"\\nNamed Entity Tree:\\n\")\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tptsVz3Rd0cx",
        "outputId": "d230e61f-2254-4173-a89a-4382121b9b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entity Tree:\n",
            "\n",
            "(S\n",
            "  On/IN\n",
            "  12/CD\n",
            "  March/NNP\n",
            "  2024/CD\n",
            "  ,/,\n",
            "  Dr./NNP\n",
            "  Anil/NNP\n",
            "  Kumar/NNP\n",
            "  from/IN\n",
            "  (ORGANIZATION IIIT/NNP Kottayam/NNP)\n",
            "  emailed/VBD\n",
            "  (PERSON Amazon/NNP India/NNP)\n",
            "  about/IN\n",
            "  a/DT\n",
            "  delayed/JJ\n",
            "  delivery/NN\n",
            "  of/IN\n",
            "  ₹4,999/NN\n",
            "  ./.\n",
            "  The/DT\n",
            "  order/NN\n",
            "  was/VBD\n",
            "  placed/VBN\n",
            "  in/IN\n",
            "  (GPE Kochi/NNP)\n",
            "  using/VBG\n",
            "  the/DT\n",
            "  Prime/NNP\n",
            "  app/NN\n",
            "  at/IN\n",
            "  10:30/CD\n",
            "  am/VBP\n",
            "  ./.\n",
            "  Later/RB\n",
            "  ,/,\n",
            "  he/PRP\n",
            "  mentioned/VBD\n",
            "  that/IN\n",
            "  (PERSON Apple/NNP Watch/NNP)\n",
            "  Series/NNP\n",
            "  8/CD\n",
            "  was/VBD\n",
            "  promised/VBN\n",
            "  within/IN\n",
            "  2/CD\n",
            "  days/NNS\n",
            "  but/CC\n",
            "  arrived/VBD\n",
            "  after/IN\n",
            "  5/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM5P9DDrdydN",
        "outputId": "f9efcb4a-69e4-4bc6-e70b-53cae11cdeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "\n",
        "for chunk in named_entities:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        entity = \" \".join(c[0] for c in chunk)\n",
        "        entity_type = chunk.label()\n",
        "        entities.append((entity, entity_type))\n",
        "\n",
        "print(\"\\nExtracted Named Entities:\\n\", entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgQ6Zi81eVhH",
        "outputId": "c983207d-a830-433b-9ee0-b43bda83642d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Named Entities:\n",
            " [('IIIT Kottayam', 'ORGANIZATION'), ('Amazon India', 'PERSON'), ('Kochi', 'GPE'), ('Apple Watch', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analysis"
      ],
      "metadata": {
        "id": "LYk1qSGXfrqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"Students are studying machine learning in the lab. The students study learning algorithms and study data every day. Studying machines learn from data, and learning improves when students keep studying.\"\n",
        "text = text.lower()"
      ],
      "metadata": {
        "id": "Yk7gTUEefwvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(\"Number of Tokens: \", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qyP6S80f1lQ",
        "outputId": "b63a87bc-7188-4e1d-9ddd-99e49fd8e513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tokens:  34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "\n",
        "print(\"\\nAfter stopword removal:\\n\", filtered_tokens[:20])\n",
        "print(\"Number of tokens after stopword removal: \", len(filtered_tokens))\n",
        "print(\"Number of Unique tokens (types): \", len(set(filtered_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yre-EvGkdCC7",
        "outputId": "90761f71-c0c8-4576-d1eb-4a80dd47dc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After stopword removal:\n",
            " ['students', 'studying', 'machine', 'learning', 'lab', 'students', 'study', 'learning', 'algorithms', 'study', 'data', 'every', 'day', 'studying', 'machines', 'learn', 'data', 'learning', 'improves', 'students']\n",
            "Number of tokens after stopword removal:  22\n",
            "Number of Unique tokens (types):  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(t) for t in filtered_tokens]\n",
        "\n",
        "print(\"\\nStemmed tokens sample:\\n\", stemmed_tokens[:20])\n",
        "print(\"Number of Unique tokens (types): \", len(set(stemmed_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu8uCNeIdNcq",
        "outputId": "bce14904-4e74-47e6-c3a8-b16534516f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed tokens sample:\n",
            " ['student', 'studi', 'machin', 'learn', 'lab', 'student', 'studi', 'learn', 'algorithm', 'studi', 'data', 'everi', 'day', 'studi', 'machin', 'learn', 'data', 'learn', 'improv', 'student']\n",
            "Number of Unique tokens (types):  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "token_freq = Counter(stemmed_tokens)\n",
        "print(\"\\nMost common tokens:\\n\", token_freq.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ra7vql5dTuN",
        "outputId": "cd0b57bf-0720-4956-c648-2986daa431f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most common tokens:\n",
            " [('studi', 5), ('learn', 4), ('student', 3), ('machin', 2), ('data', 2), ('lab', 1), ('algorithm', 1), ('everi', 1), ('day', 1), ('improv', 1)]\n"
          ]
        }
      ]
    }
  ]
}